{
  "story_id": "12",
  "phase": "a",
  "status": "passed",
  "completed_at": "2026-02-12T21:05:00Z",
  "planning_output": {
    "implementation_plan": "## Story 12: MCP Tools - Batch Operations\n\nImplement three batch tool functions (`async_batch_read`, `async_batch_write`, `async_batch_update`) that delegate to the existing single-file tools for each item in the batch. Operations are NOT transactional - partial failures are reported per-file in the results array. Each batch function returns a summary with total/succeeded/failed/contention counts.\n\n### Architecture\n\nAll request/response models already exist (stories 9-11 created `BatchReadItem`, `BatchWriteItem`, `BatchUpdateItem`, `AsyncBatchReadRequest`, `AsyncBatchWriteRequest`, `AsyncBatchUpdateRequest`, `BatchSummary`, `BatchReadResponse`, `BatchWriteResponse`, `BatchUpdateResponse`). The implementation phase creates three new tool files that iterate over the items, call the existing single-file tools, collect results, and build the summary.\n\n### Key Design Decisions\n\n1. **Sequential per-file processing**: Each file in the batch is processed sequentially (not concurrently) to avoid deadlock scenarios with the lock manager. This matches the existing pattern where each tool acquires/releases locks per file.\n\n2. **Delegation pattern**: Each batch function calls the corresponding single-file function (`async_read`, `async_write`, `async_update`) internally, wrapping each `BatchXxxItem` into the equivalent `AsyncXxxRequest`. This maximizes code reuse and ensures identical per-file behavior.\n\n3. **Non-transactional semantics**: If file N fails, files N+1..M still execute. The response always includes all per-file results plus a summary.\n\n4. **Contention tracking (batch_update only)**: The summary's `contention` field counts `ContentionResponse` results, distinct from `failed` (which counts `ErrorResponse` results).\n\n5. **Function signatures follow existing conventions**: Each batch function takes `request`, `path_validator`, `lock_manager`, and (for write/update) `hash_registry` as parameters.",
    "files_to_modify": [
      "src/async_crud_mcp/tools/async_batch_read.py",
      "src/async_crud_mcp/tools/async_batch_write.py",
      "src/async_crud_mcp/tools/async_batch_update.py",
      "src/async_crud_mcp/tools/__init__.py",
      "tests/test_tools/test_async_batch_read.py",
      "tests/test_tools/test_async_batch_write.py",
      "tests/test_tools/test_async_batch_update.py"
    ],
    "estimated_complexity": "medium",
    "ac_breakdown": {
      "AC-12.1": "**batch_read: multiple files in single call**\n\nCreate `src/async_crud_mcp/tools/async_batch_read.py` with function `async_batch_read(request: AsyncBatchReadRequest, path_validator: PathValidator, lock_manager: LockManager) -> BatchReadResponse`.\n\nImplementation:\n1. Iterate over `request.files` (list of `BatchReadItem`)\n2. For each item, construct an `AsyncReadRequest(path=item.path, offset=item.offset, limit=item.limit, encoding=item.encoding)`\n3. Call `async_read(read_request, path_validator, lock_manager)`\n4. Collect each result into a `results` list\n5. Count succeeded (status='ok') and failed (status='error')\n6. Return `BatchReadResponse(results=results, summary=BatchSummary(total=len(files), succeeded=ok_count, failed=error_count, contention=0))`\n\nWrap the entire function in a try/except for unexpected errors, returning a BatchReadResponse with all items as errors if something catastrophic happens.",
      "AC-12.2": "**batch_write: multiple new files**\n\nCreate `src/async_crud_mcp/tools/async_batch_write.py` with function `async_batch_write(request: AsyncBatchWriteRequest, path_validator: PathValidator, lock_manager: LockManager, hash_registry: HashRegistry) -> BatchWriteResponse`.\n\nImplementation:\n1. Iterate over `request.files` (list of `BatchWriteItem`)\n2. For each item, construct an `AsyncWriteRequest(path=item.path, content=item.content, encoding=item.encoding, create_dirs=item.create_dirs, timeout=request.timeout)`\n3. Call `async_write(write_request, path_validator, lock_manager, hash_registry)`\n4. Collect each result into a `results` list\n5. Count succeeded and failed\n6. Return `BatchWriteResponse(results=results, summary=BatchSummary(total=len(files), succeeded=ok_count, failed=error_count, contention=0))`",
      "AC-12.3": "**batch_update: per-file contention resolution**\n\nCreate `src/async_crud_mcp/tools/async_batch_update.py` with function `async_batch_update(request: AsyncBatchUpdateRequest, path_validator: PathValidator, lock_manager: LockManager, hash_registry: HashRegistry) -> BatchUpdateResponse`.\n\nImplementation:\n1. Iterate over `request.files` (list of `BatchUpdateItem`)\n2. For each item, construct an `AsyncUpdateRequest(path=item.path, expected_hash=item.expected_hash, content=item.content, patches=item.patches, encoding=item.encoding, timeout=request.timeout, diff_format=request.diff_format)`\n3. Call `async_update(update_request, path_validator, lock_manager, hash_registry)`\n4. Collect each result; classify as succeeded (UpdateSuccessResponse), contention (ContentionResponse), or failed (ErrorResponse)\n5. Return `BatchUpdateResponse(results=results, summary=BatchSummary(total=len(files), succeeded=ok_count, failed=error_count, contention=contention_count))`\n\nContention is tracked separately from failures - a ContentionResponse is NOT a failure, it's a distinct status indicating the file changed since the expected hash.",
      "AC-12.4": "**NOT transactional: partial failures reported per-file**\n\nAll three batch functions process files sequentially in a loop. If any individual file operation returns an `ErrorResponse`, it is added to the results array and the loop continues to the next file. There is no rollback mechanism. Each file operation is independent.\n\nThe outer try/except in each batch function catches truly unexpected errors (e.g., runtime crash) and returns a summary with all remaining items marked as failed.",
      "AC-12.5": "**Summary with total/succeeded/failed/contention counts**\n\nThe `BatchSummary` model already exists with fields: `total`, `succeeded`, `failed`, `contention`. After processing all files:\n- `total` = len(request.files)\n- `succeeded` = count of results with status='ok'\n- `failed` = count of results with status='error'\n- `contention` = count of results with status='contention' (batch_update only; 0 for read/write)\n\nThe summary is included in every batch response regardless of success/failure."
    },
    "testing_strategy": "Create three test files mirroring the existing test pattern:\n\n**tests/test_tools/test_async_batch_read.py**:\n- Test reading multiple files in single call (2-3 files, verify all contents)\n- Test partial failure (one valid file, one nonexistent file) - verify succeeded=1, failed=1\n- Test empty batch (0 files) - verify total=0, succeeded=0\n- Test all files fail (all paths outside base) - verify failed=N\n- Test with offset/limit per file\n\n**tests/test_tools/test_async_batch_write.py**:\n- Test writing multiple new files (2-3 files, verify all created with correct content/hash)\n- Test partial failure (one valid path, one existing file) - verify succeeded=1, failed=1\n- Test empty batch\n- Test with create_dirs=True for nested paths\n\n**tests/test_tools/test_async_batch_update.py**:\n- Test updating multiple files successfully (correct expected_hash for each)\n- Test partial failure (one valid update, one with wrong hash) - verify succeeded=1, contention=1\n- Test mixed results (success + contention + error) - verify all three counts\n- Test empty batch\n- Test with patches mode (not just content mode)\n\nAll tests use the same fixtures pattern: temp_base_dir, path_validator, lock_manager, hash_registry, pytest.mark.asyncio.",
    "dependencies": []
  }
}
